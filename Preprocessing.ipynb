{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "from data.abc import ABCPreProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be preprocessing two types of data --> **ABC Notation** data and **Audio** data\n",
    "\n",
    "### ABC Notation Data\n",
    "\n",
    "- Strip away **Tune body**, **key**, **meter**, **rhythm** and store all other fields of an ABC track as metadata\n",
    "- Use key and meter as conditioning symbols when generating a tune\n",
    "- Tokenize according to vocabulary of musical transcription tokens\n",
    "\n",
    "- Create a TFRecord Dataset consisting sequence examples like --> **[ tune, meter, key, rhythm ]**\n",
    "\n",
    "### Audio Data\n",
    "- Turning the full audio into short examples (4 seconds by default, but adjustable with flags)\n",
    "- Inferring the fundamental frequency (or \"pitch\") with CREPE\n",
    "- Computing the loudness features\n",
    "\n",
    "- Create TFRecord Dataset consisting sequence examples like --> **[ Audio, f0_feature, loudness_feature ]**\n",
    "\n",
    "#### Each tune be indexed such that using its ID, we can find its ABC Notation as well as related audio files\n",
    "- A tune can be associated with more than one audio file (Different audio lengths!!)\n",
    "\n",
    "At the end of the file, we should merge both the datasets, to obtain a single TFRecord file containing preprocessed ABC data and preprocessed audio files indexed according to the different tunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mention the path to the datastore\n",
    "BASE_DIR = \"/home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/\"\n",
    "ABC_DATA_DIR = os.path.join(BASE_DIR, \"datasets\", \"abc_data\")\n",
    "AUDIO_DATA_DIR = os.path.join(BASE_DIR, \"datasets\", \"audio\")\n",
    "ABC_TFRECORD_DIR = os.path.join(BASE_DIR, \"tfrecords\", 'abc')\n",
    "AUDIO_TFRECORD_DIR = os.path.join(BASE_DIR, \"tfrecords\", 'audio')\n",
    "PROCESSED_ABC_FILENAME = 'processed-abc-files'\n",
    "PROCESSED_AUDIO_FILENAME = 'processed-audio-files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - ABC Notation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To understand the underlying distribution of tunes in Irish music, it can be helpful to visualize the following quantities:\n",
    "- Maximum length of tunes in each category\n",
    "- Number of tunes in each category\n",
    "- Number of tunes in each key\n",
    "- Number of tunes in each meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cool. Lets process these ABC files now!\n",
      "Processing files and writing extracted information to CSV for easy processing from here onwards ...\n",
      "CSV PATH --> /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/tfrecords/abc/processed-abc-files.csv\n",
      "Found 2 files in the ABC Data Directory. Looking into these files now ..\n",
      "---------------------------------------------------------\n",
      "0. /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/datasets/abc_data/thesessions_data_cleaned.abc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 847/151041 [00:00<00:17, 8463.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information and storing to CSV now ..\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151041/151041 [00:14<00:00, 10486.10it/s]\n",
      "100%|██████████| 366/366 [00:00<00:00, 7217.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored tunes from file /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/datasets/abc_data/thesessions_data_cleaned.abc to CSV!\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "1. /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/datasets/abc_data/double_jigs_cleaned.abc\n",
      "Extracting information and storing to CSV now ..\n",
      "---------------------------------------------------------\n",
      "Stored tunes from file /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/datasets/abc_data/double_jigs_cleaned.abc to CSV!\n",
      "---------------------------------------------------------\n",
      "Number of tunes - 139193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "  0%|          | 0/139194 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "ABC Extended Vocabulary:\n",
      "{'word_to_idx': {'C,': '1', '^C,': '2', 'D,': '3', '^D,': '4', 'E,': '5', 'F,': '6', '^F,': '7', 'G,': '8', '^G,': '9', 'A,': '10', '^A,': '11', 'B,': '12', 'C': '204', '^C': '14', 'D': '191', '^D': '16', 'E': '17', 'F': '187', '^F': '19', 'G': '202', '^G': '21', 'A': '214', '^A': '23', 'B': '225', 'c': '25', '^c': '26', 'd': '27', '^d': '28', 'e': '29', 'f': '30', '^f': '31', 'g': '32', '^g': '33', 'a': '34', '^a': '35', 'b': '36', \"c'\": '37', \"^c'\": '38', \"d'\": '39', \"^d'\": '40', \"e'\": '41', \"f'\": '42', \"g'\": '43', \"^g'\": '44', \"a'\": '45', \"^a'\": '46', \"B'\": '47', '_C,': '48', '_D,': '49', '_E,': '50', '_G,': '51', '_A,': '52', '_B,': '53', '_C': '54', '_D': '55', '_E': '56', '_G': '57', '_A': '58', '_B': '59', '_c': '60', '_d': '61', '_e': '62', '_g': '63', '_a': '64', '_b': '65', \"_c'\": '66', \"_d'\": '67', \"_e'\": '68', \"_g'\": '69', \"_a'\": '70', \"_b'\": '71', '=C,': '72', '=E,': '73', '=F,': '74', '=G,': '75', '=A,': '76', '=B,': '77', '=C': '78', '=D': '79', '=E': '80', '=F': '81', '=G': '82', '=A': '83', '=B': '84', '=c': '85', '=d': '86', '=e': '87', '=f': '88', '=g': '89', '=a': '90', '=b': '91', \"=c'\": '92', \"=d'\": '93', \"=e'\": '94', \"=f'\": '95', \"=g'\": '96', \"=a'\": '97', \"=b'\": '98', 'z': '99', '[': '100', ']': '101', '{': '102', '}': '103', '(': '104', ')': '105', '~': '106', '.': '107', '|': '108', '|:': '109', ':|': '110', '|]': '111', '||': '112', '[|': '113', '::': '114', '|1': '115', '|2': '116', '(2': '117', '(3': '118', '(4': '119', '(5': '120', '(6': '121', '(7': '122', '(9': '123', '/2': '124', '/3': '125', '/4': '126', '/8': '127', '2': '128', '3': '129', '4': '130', '5': '131', '6': '132', '7': '133', '8': '134', '9': '135', '12': '136', '16': '137', '3/4': '154', '3/2': '158', '5/2': '140', '7/2': '141', '2>': '142', '2<': '143', '/2>': '144', '/2<': '145', '4>': '146', '<': '147', '>': '148', '<s>': '149', '</s>': '150', '4/4': '151', '9/8': '152', '6/8': '153', '2/4': '155', '12/8': '156', '6/4': '157', '5/4': '159', '7/8': '160', '11/8': '161', '7/4': '162', '5/8': '163', 'C|': '165', '2/2': '166', '26/8': '167', '13/8': '168', '8/8': '169', '9/4': '170', 'Emin': '171', 'Edor': '172', 'Gmaj': '173', 'Dmaj': '174', 'Amin': '175', 'Ador': '176', 'Cmaj': '177', 'Gdor': '178', 'Amaj': '179', 'Fmaj': '180', 'Amix': '181', 'Bmin': '182', 'Gmin': '183', 'Dmix': '184', 'Ddor': '185', 'DMaj': '186', 'Dmin': '188', 'Fdor': '189', 'Bb': '190', 'Emaj': '192', 'Cdor': '193', 'Gmix': '194', 'DMix': '195', 'GMaj': '196', 'Dm': '197', 'Bdor': '198', 'EbMaj': '199', 'Bm': '200', 'Emix': '201', 'Bbmin': '203', 'BbMaj': '205', 'CMaj': '206', 'Am': '207', 'Bmix': '208', 'GLyd': '209', 'Eb': '210', 'EDor': '211', 'Aminor': '212', 'BbMajor': '213', 'Cmin': '215', 'ALydian': '216', 'FMajor': '217', 'GMajor': '218', 'DMixolydian': '219', 'AMix': '220', 'DMajor': '221', 'Gmajor': '222', 'AMaj': '223', 'ADor': '224', 'Fmin': '226', 'EMaj': '227', 'Gm': '228', 'AMajor': '229', 'GMin': '230', 'Em': '231', 'Cmix': '232'}, 'idx_to_word': {'1': 'C,', '2': '^C,', '3': 'D,', '4': '^D,', '5': 'E,', '6': 'F,', '7': '^F,', '8': 'G,', '9': '^G,', '10': 'A,', '11': '^A,', '12': 'B,', '13': 'C', '14': '^C', '15': 'D', '16': '^D', '17': 'E', '18': 'F', '19': '^F', '20': 'G', '21': '^G', '22': 'A', '23': '^A', '24': 'B', '25': 'c', '26': '^c', '27': 'd', '28': '^d', '29': 'e', '30': 'f', '31': '^f', '32': 'g', '33': '^g', '34': 'a', '35': '^a', '36': 'b', '37': \"c'\", '38': \"^c'\", '39': \"d'\", '40': \"^d'\", '41': \"e'\", '42': \"f'\", '43': \"g'\", '44': \"^g'\", '45': \"a'\", '46': \"^a'\", '47': \"B'\", '48': '_C,', '49': '_D,', '50': '_E,', '51': '_G,', '52': '_A,', '53': '_B,', '54': '_C', '55': '_D', '56': '_E', '57': '_G', '58': '_A', '59': '_B', '60': '_c', '61': '_d', '62': '_e', '63': '_g', '64': '_a', '65': '_b', '66': \"_c'\", '67': \"_d'\", '68': \"_e'\", '69': \"_g'\", '70': \"_a'\", '71': \"_b'\", '72': '=C,', '73': '=E,', '74': '=F,', '75': '=G,', '76': '=A,', '77': '=B,', '78': '=C', '79': '=D', '80': '=E', '81': '=F', '82': '=G', '83': '=A', '84': '=B', '85': '=c', '86': '=d', '87': '=e', '88': '=f', '89': '=g', '90': '=a', '91': '=b', '92': \"=c'\", '93': \"=d'\", '94': \"=e'\", '95': \"=f'\", '96': \"=g'\", '97': \"=a'\", '98': \"=b'\", '99': 'z', '100': '[', '101': ']', '102': '{', '103': '}', '104': '(', '105': ')', '106': '~', '107': '.', '108': '|', '109': '|:', '110': ':|', '111': '|]', '112': '||', '113': '[|', '114': '::', '115': '|1', '116': '|2', '117': '(2', '118': '(3', '119': '(4', '120': '(5', '121': '(6', '122': '(7', '123': '(9', '124': '/2', '125': '/3', '126': '/4', '127': '/8', '128': '2', '129': '3', '130': '4', '131': '5', '132': '6', '133': '7', '134': '8', '135': '9', '136': '12', '137': '16', '138': '3/4', '139': '3/2', '140': '5/2', '141': '7/2', '142': '2>', '143': '2<', '144': '/2>', '145': '/2<', '146': '4>', '147': '<', '148': '>', '149': '<s>', '150': '</s>', '151': '4/4', '152': '9/8', '153': '6/8', '154': '3/4', '155': '2/4', '156': '12/8', '157': '6/4', '158': '3/2', '159': '5/4', '160': '7/8', '161': '11/8', '162': '7/4', '163': '5/8', '164': 'C', '165': 'C|', '166': '2/2', '167': '26/8', '168': '13/8', '169': '8/8', '170': '9/4', '171': 'Emin', '172': 'Edor', '173': 'Gmaj', '174': 'Dmaj', '175': 'Amin', '176': 'Ador', '177': 'Cmaj', '178': 'Gdor', '179': 'Amaj', '180': 'Fmaj', '181': 'Amix', '182': 'Bmin', '183': 'Gmin', '184': 'Dmix', '185': 'Ddor', '186': 'DMaj', '187': 'F', '188': 'Dmin', '189': 'Fdor', '190': 'Bb', '191': 'D', '192': 'Emaj', '193': 'Cdor', '194': 'Gmix', '195': 'DMix', '196': 'GMaj', '197': 'Dm', '198': 'Bdor', '199': 'EbMaj', '200': 'Bm', '201': 'Emix', '202': 'G', '203': 'Bbmin', '204': 'C', '205': 'BbMaj', '206': 'CMaj', '207': 'Am', '208': 'Bmix', '209': 'GLyd', '210': 'Eb', '211': 'EDor', '212': 'Aminor', '213': 'BbMajor', '214': 'A', '215': 'Cmin', '216': 'ALydian', '217': 'FMajor', '218': 'GMajor', '219': 'DMixolydian', '220': 'AMix', '221': 'DMajor', '222': 'Gmajor', '223': 'AMaj', '224': 'ADor', '225': 'B', '226': 'Fmin', '227': 'EMaj', '228': 'Gm', '229': 'AMajor', '230': 'GMin', '231': 'Em', '232': 'Cmix'}}\n",
      "---------------------------------------------------------\n",
      "Preparing to save extracted information into a TFRecord file at /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/tfrecords/abc/processed-abc-files.tfrecord ...\n",
      "Creating TFRecord File ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 86/139194 [00:16<7:13:17,  5.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0718137eadf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABC_DATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m tfrecord_path = preprocessor.save_as_tfrecord_dataset(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABC_TFRECORD_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tunes_vocab.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/AI-Music-Generation-Challenge-2020/data/abc.py\u001b[0m in \u001b[0;36msave_as_tfrecord_dataset\u001b[0;34m(self, vocab_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CONSTANT'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                             ),\n\u001b[0;32m--> 131\u001b[0;31m                             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_tune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                         )\n\u001b[1;32m    133\u001b[0m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/AI-Music-Generation-Challenge-2020/data/abc.py\u001b[0m in \u001b[0;36mserialize_example\u001b[0;34m(_input, _output, tune_length)\u001b[0m\n\u001b[1;32m    340\u001b[0m                     feature = [\n\u001b[1;32m    341\u001b[0m                         tf.train.Feature(\n\u001b[0;32m--> 342\u001b[0;31m                             \u001b[0mint64_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt64List\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                         )\n\u001b[1;32m    344\u001b[0m                     ]  \n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6928\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6929\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6930\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6931\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6932\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    997\u001b[0m       skip_on_eager=False) as name:\n\u001b[1;32m    998\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       packed_begin, packed_end, packed_strides = (stack(begin), stack(end),\n\u001b[0m\u001b[1;32m   1000\u001b[0m                                                   stack(strides))\n\u001b[1;32m   1001\u001b[0m       if (packed_begin.dtype == dtypes.int64 or\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_autopacking_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m   \u001b[0;34m\"\"\"Tensor conversion function that automatically packs arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mas_ref\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_should_not_autopack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m   \u001b[0minferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dtype_from_nested_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_should_not_autopack\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m   1483\u001b[0m   \u001b[0;31m# pylint: disable=unidiomatic-typecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m   \u001b[0;31m# TODO(slebedev): add nest.all?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_NON_AUTOPACKABLE_TYPES\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m   \u001b[0;31m# pylint: enable=unidiomatic-typecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/projects/AI_Music_Challenge_2020/ai_music_venv/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m   \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocessor = ABCPreProcessor(ABC_TFRECORD_DIR, PROCESSED_ABC_FILENAME)\n",
    "json_path = preprocessor.process(ABC_DATA_DIR)\n",
    "tfrecord_path = preprocessor.save_as_tfrecord_dataset(\n",
    "    os.path.join(ABC_TFRECORD_DIR, 'tunes_vocab.json')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ABCPreProcessor' object has no attribute 'visualize_stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d0e7a4b51e65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dataset = preprocessor.load_tfrecord_dataset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ABCPreProcessor' object has no attribute 'visualize_stats'"
     ]
    }
   ],
   "source": [
    "preprocessor.visualize_stats()\n",
    "\n",
    "#dataset = preprocessor.load_tfrecord_dataset()\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
