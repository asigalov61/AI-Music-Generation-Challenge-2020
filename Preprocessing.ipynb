{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ddsp\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib\n",
    "import os\n",
    "import glob\n",
    "from data.abc import ABCPreProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be preprocessing two types of data --> **ABC Notation** data and **Audio** data\n",
    "\n",
    "### ABC Notation Data\n",
    "\n",
    "- Strip away **Tune body**, **key**, **meter** and store all other fields of an ABC track as metadata\n",
    "- Use key and meter as conditioning symbols when generating a tune\n",
    "- Tokenize according to vocabulary of musical transcription tokens\n",
    "\n",
    "- Create a TFRecord Dataset consisting sequence examples like --> **[ One-hot encoded tune body, meter, key ]**\n",
    "\n",
    "### Audio Data\n",
    "- Turning the full audio into short examples (4 seconds by default, but adjustable with flags)\n",
    "- Inferring the fundamental frequency (or \"pitch\") with CREPE\n",
    "- Computing the loudness features\n",
    "\n",
    "- Create TFRecord Dataset consisting sequence examples like --> **[ Audio, f0_feature, loudness_feature ]**\n",
    "\n",
    "#### Each tune be indexed such that using its ID, we can find its ABC Notation as well as related audio files\n",
    "- A tune can be associated with more than one audio file (Different audio lengths!!)\n",
    "\n",
    "At the end of the file, we should merge both the datasets, to obtain a single TFRecord file containing preprocessed ABC data and preprocessed audio files indexed according to the different tunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mention the path to the datastore\n",
    "BASE_DIR = \"/home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/\"\n",
    "ABC_DATA_DIR = os.path.join(BASE_DIR, \"datasets\", \"abc_data\")\n",
    "AUDIO_DATA_DIR = os.path.join(BASE_DIR, \"datasets\", \"audio\")\n",
    "ABC_TFRECORD_DIR = os.path.join(BASE_DIR, \"tfrecords\", 'abc')\n",
    "AUDIO_TFRECORD_DIR = os.path.join(BASE_DIR, \"tfrecords\", 'audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - ABC Notation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data: 3127\n",
      "Size of Vocabulary: 68\n",
      "{'[', '^', 'M', 'J', '(', '_', '|', 'B', \"'\", 'l', 't', 'm', '<', 'P', '=', 'o', 'r', 'z', '~', 'f', '2', '>', 'v', '.', '3', 'k', '{', '<s>', ',', '#', '1', '8', '}', 'e', '7', 'T', 'g', 'D', 'E', 'b', 'S', 'c', '-', 'p', 'F', 'A', '6', 'n', 'Q', 'd', 'G', '4', 'H', '\\\\', ' ', ']', '\"', 'h', 'O', 's', 'C', ':', 'i', ')', 'a', 'L', '/', '9'}\n",
      "Number of modal keys: 26\n",
      "{'Dm', 'Emix', 'Ddor', 'Edor', 'Am', 'Bmix', 'Gdor', 'Dmix', 'Em', 'Amix', 'Bdor', 'D', 'Ador', 'E', 'Glyd', 'F', 'A', 'Cmix', 'F#m', 'G', 'Gm', 'F#dor', 'C', 'E ', 'Bm', 'Gmix'}\n",
      "Number of musical meters: 8\n",
      "{'3/2', '2/4', '4/4', '6/8', '3/4', '9/8', '6/4', '2/2'}\n",
      "Number of rhythms: 23\n",
      "{'single jig', 'mazurka', 'set dance', 'slide', 'waltz', 'hop jig', 'fling', 'polka', 'slip jig', 'Double jig, march', 'jig', 'Double jig', 'hornpipe', 'reel', 'song', 'march', 'slow air', 'air', 'carolan', 'country dance', 'barndance', 'strathspey', 'highland'}\n",
      "{'Dm': 0, 'Emix': 1, 'Ddor': 2, 'Edor': 3, 'Am': 4, 'Bmix': 5, 'Gdor': 6, 'Dmix': 7, 'Em': 8, 'Amix': 9, 'Bdor': 10, 'D': 11, 'Ador': 12, 'E': 13, 'Glyd': 14, 'F': 15, 'A': 16, 'Cmix': 17, 'F#m': 18, 'G': 19, 'Gm': 20, 'F#dor': 21, 'C': 22, 'E ': 23, 'Bm': 24, 'Gmix': 25}\n",
      "{'3/2': 0, '2/4': 1, '4/4': 2, '6/8': 3, 'C': 4, '3/4': 5, '9/8': 6, 'C|': 7, '6/4': 8, '2/2': 9}\n",
      "{'single jig': 0, 'mazurka': 1, 'set dance': 2, 'slide': 3, 'waltz': 4, 'hop jig': 5, 'fling': 6, 'polka': 7, 'slip jig': 8, 'Double jig, march': 9, 'jig': 10, 'Double jig': 11, 'hornpipe': 12, 'reel': 13, 'song': 14, 'march': 15, 'slow air': 16, 'air': 17, 'carolan': 18, 'country dance': 19, 'barndance': 20, 'strathspey': 21, 'highland': 22}\n",
      "<TensorSliceDataset shapes: ((1434,), (), (), ()), types: (tf.int32, tf.int32, tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ABCPreProcessor(ABC_DATA_DIR, ABC_TFRECORD_DIR, 'processed-abc-files')\n",
    "# Stores extracted information in a structured JSON file\n",
    "#preprocessor.process()\n",
    "\n",
    "# Compute features and create a dataset\n",
    "preprocessor.calculate_statistics()\n",
    "\n",
    "preprocessor.save_as_tfrecord_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
