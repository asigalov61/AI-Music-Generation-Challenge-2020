{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "from data.abc import ABCPreProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be preprocessing two types of data --> **ABC Notation** data and **Audio** data\n",
    "\n",
    "### ABC Notation Data\n",
    "\n",
    "- Strip away **Tune body**, **key**, **meter**, **rhythm** and store all other fields of an ABC track as metadata\n",
    "- Use key and meter as conditioning symbols when generating a tune\n",
    "- Tokenize according to vocabulary of musical transcription tokens\n",
    "\n",
    "- Create a TFRecord Dataset consisting sequence examples like --> **[ tune, meter, key, rhythm ]**\n",
    "\n",
    "At the end of the file, we should merge both the datasets, to obtain a single TFRecord file containing preprocessed ABC data indexed according to the different tunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mention the path to the datastore\n",
    "BASE_DIR = \"/home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/AI-Music-Generation-Challenge-2020\"\n",
    "PROJECT_DIR = \"/home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020\"\n",
    "ABC_TFRECORD_DIR = os.path.join(PROJECT_DIR, \"tfrecords\", \"abc\")\n",
    "ABC_DATA_DIR = os.path.join(PROJECT_DIR, 'datasets', 'abc_data')\n",
    "PROCESSED_ABC_FILENAME = 'processed-abc-files'\n",
    "\n",
    "#BASE_DIR = \"/home/rithomas/\"\n",
    "#ABC_DATA_DIR = os.path.join(BASE_DIR, \"data\", \"ABC\")\n",
    "#ABC_TFRECORD_DIR = os.path.join(BASE_DIR, \"cache\", \"ABC\", \"Double-Jigs\")\n",
    "#PROCESSED_ABC_FILENAME = 'processed-abc-files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - ABC Notation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To understand the underlying distribution of tunes in Irish music, it can be helpful to visualize the following quantities:\n",
    "- Maximum length of tunes in each category\n",
    "- Number of tunes in each category\n",
    "- Number of tunes in each key\n",
    "- Number of tunes in each meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [00:00<00:00, 5652.37it/s]\n",
      "  0%|          | 0/366 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cool. Lets process these ABC files now!\n",
      "Processing files and writing extracted information to CSV for easy processing from here onwards ...\n",
      "CSV PATH --> /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/tfrecords/abc/processed-abc-files.csv\n",
      "Found 1 files in the ABC Data Directory. Looking into these files now ..\n",
      "---------------------------------------------------------\n",
      "0. /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/datasets/abc_data/double_jigs.abc\n",
      "Extracting information and storing to CSV now ..\n",
      "---------------------------------------------------------\n",
      "Stored tunes from file /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/datasets/abc_data/double_jigs.abc to CSV!\n",
      "---------------------------------------------------------\n",
      "Number of tunes - 365\n",
      "---------------------------------------------------------\n",
      "ABC Extended Vocabulary:\n",
      "{'word_to_idx': {'C,': '1', '^C,': '2', 'D,': '3', '^D,': '4', 'E,': '5', 'F,': '6', '^F,': '7', 'G,': '8', '^G,': '9', 'A,': '10', '^A,': '11', 'B,': '12', 'C': '13', '^C': '14', 'D': '15', '^D': '16', 'E': '17', 'F': '18', '^F': '19', 'G': '20', '^G': '21', 'A': '22', '^A': '23', 'B': '24', 'c': '25', '^c': '26', 'd': '27', '^d': '28', 'e': '29', 'f': '30', '^f': '31', 'g': '32', '^g': '33', 'a': '34', '^a': '35', 'b': '36', \"c'\": '37', \"^c'\": '38', \"d'\": '39', \"^d'\": '40', \"e'\": '41', \"f'\": '42', \"g'\": '43', \"^g'\": '44', \"a'\": '45', \"^a'\": '46', \"B'\": '47', '_C,': '48', '_D,': '49', '_E,': '50', '_G,': '51', '_A,': '52', '_B,': '53', '_C': '54', '_D': '55', '_E': '56', '_G': '57', '_A': '58', '_B': '59', '_c': '60', '_d': '61', '_e': '62', '_g': '63', '_a': '64', '_b': '65', \"_c'\": '66', \"_d'\": '67', \"_e'\": '68', \"_g'\": '69', \"_a'\": '70', \"_b'\": '71', '=C,': '72', '=E,': '73', '=F,': '74', '=G,': '75', '=A,': '76', '=B,': '77', '=C': '78', '=D': '79', '=E': '80', '=F': '81', '=G': '82', '=A': '83', '=B': '84', '=c': '85', '=d': '86', '=e': '87', '=f': '88', '=g': '89', '=a': '90', '=b': '91', \"=c'\": '92', \"=d'\": '93', \"=e'\": '94', \"=f'\": '95', \"=g'\": '96', \"=a'\": '97', \"=b'\": '98', 'z': '99', '|': '100', '|:': '101', ':|': '102', '|]': '103', '||': '104', '[|': '105', '::': '106', '|1': '107', '|2': '108', '(2': '109', '(3': '110', '(4': '111', '(5': '112', '(6': '113', '(7': '114', '(9': '115', '/': '116', '/2': '117', '/3': '118', '/4': '119', '/8': '120', '2': '121', '3': '122', '4': '123', '5': '124', '6': '125', '7': '126', '8': '127', '9': '128', '12': '129', '16': '130', '3/4': '131', '3/2': '132', '5/2': '133', '7/2': '134', '2>': '135', '2<': '136', '/2>': '137', '/2<': '138', '4>': '139', '<': '140', '>': '141', '<s>': '142', '</s>': '143', 'M:6/8': '144', 'K:D': '145', 'K:A': '146', 'K:G': '147', 'K:Am': '148', 'K:Ddor': '149', 'K:Amix': '150', 'K:Em': '151', 'K:Bm': '152', 'K:Dmix': '153', 'K:Edor': '154', 'K:Ador': '155', 'K:C': '156', 'K:Dm': '157', 'K:Gdor': '158', 'K:F': '159', 'K:Gm': '160'}, 'idx_to_word': {'1': 'C,', '2': '^C,', '3': 'D,', '4': '^D,', '5': 'E,', '6': 'F,', '7': '^F,', '8': 'G,', '9': '^G,', '10': 'A,', '11': '^A,', '12': 'B,', '13': 'C', '14': '^C', '15': 'D', '16': '^D', '17': 'E', '18': 'F', '19': '^F', '20': 'G', '21': '^G', '22': 'A', '23': '^A', '24': 'B', '25': 'c', '26': '^c', '27': 'd', '28': '^d', '29': 'e', '30': 'f', '31': '^f', '32': 'g', '33': '^g', '34': 'a', '35': '^a', '36': 'b', '37': \"c'\", '38': \"^c'\", '39': \"d'\", '40': \"^d'\", '41': \"e'\", '42': \"f'\", '43': \"g'\", '44': \"^g'\", '45': \"a'\", '46': \"^a'\", '47': \"B'\", '48': '_C,', '49': '_D,', '50': '_E,', '51': '_G,', '52': '_A,', '53': '_B,', '54': '_C', '55': '_D', '56': '_E', '57': '_G', '58': '_A', '59': '_B', '60': '_c', '61': '_d', '62': '_e', '63': '_g', '64': '_a', '65': '_b', '66': \"_c'\", '67': \"_d'\", '68': \"_e'\", '69': \"_g'\", '70': \"_a'\", '71': \"_b'\", '72': '=C,', '73': '=E,', '74': '=F,', '75': '=G,', '76': '=A,', '77': '=B,', '78': '=C', '79': '=D', '80': '=E', '81': '=F', '82': '=G', '83': '=A', '84': '=B', '85': '=c', '86': '=d', '87': '=e', '88': '=f', '89': '=g', '90': '=a', '91': '=b', '92': \"=c'\", '93': \"=d'\", '94': \"=e'\", '95': \"=f'\", '96': \"=g'\", '97': \"=a'\", '98': \"=b'\", '99': 'z', '100': '|', '101': '|:', '102': ':|', '103': '|]', '104': '||', '105': '[|', '106': '::', '107': '|1', '108': '|2', '109': '(2', '110': '(3', '111': '(4', '112': '(5', '113': '(6', '114': '(7', '115': '(9', '116': '/', '117': '/2', '118': '/3', '119': '/4', '120': '/8', '121': '2', '122': '3', '123': '4', '124': '5', '125': '6', '126': '7', '127': '8', '128': '9', '129': '12', '130': '16', '131': '3/4', '132': '3/2', '133': '5/2', '134': '7/2', '135': '2>', '136': '2<', '137': '/2>', '138': '/2<', '139': '4>', '140': '<', '141': '>', '142': '<s>', '143': '</s>', '144': 'M:6/8', '145': 'K:D', '146': 'K:A', '147': 'K:G', '148': 'K:Am', '149': 'K:Ddor', '150': 'K:Amix', '151': 'K:Em', '152': 'K:Bm', '153': 'K:Dmix', '154': 'K:Edor', '155': 'K:Ador', '156': 'K:C', '157': 'K:Dm', '158': 'K:Gdor', '159': 'K:F', '160': 'K:Gm'}}\n",
      "---------------------------------------------------------\n",
      "Preparing to save extracted information into a TFRecord file at /home/richhiey/Desktop/workspace/projects/AI_Music_Challenge_2020/tfrecords/abc/processed-abc-files.tfrecord ...\n",
      "Creating TFRecord File ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 365/366 [00:29<00:00, 12.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving to TFRecord Dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ABCPreProcessor(ABC_TFRECORD_DIR, PROCESSED_ABC_FILENAME)\n",
    "csv_path = preprocessor.process(ABC_DATA_DIR)\n",
    "tokenizer = preprocessor.create_tokenizer()\n",
    "print('---------------------------------------------------------')\n",
    "print('ABC Extended Vocabulary:')\n",
    "print(tokenizer.return_vocabulary())\n",
    "print('---------------------------------------------------------')\n",
    "tfrecord_path = preprocessor.save_as_tfrecord_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
